{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `giotto-tda` persistent homology tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Vietoris-Rips and Pipeline API quick-start,\n",
    "2. Time series and model selection - gravitational waves classification using the Sliding windows Embedding, and caching for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vietoris-Rips API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a dataset of circles, spheres and toris, 10 samples for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.generate_datasets import make_point_clouds\n",
    "n_samples_per_class = 10\n",
    "point_clouds, labels = make_point_clouds(n_samples_per_class, 10, 0.1)\n",
    "\n",
    "print(f\"(n_point_clouds, n_points, dimension) = {point_clouds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.plotting import plot_point_cloud\n",
    "plot_point_cloud(point_clouds[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.homology import VietorisRipsPersistence\n",
    "\n",
    "VR = VietorisRipsPersistence(homology_dimensions=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(\n",
    "- import from the homology module, where many classes are available - WeightedAlpha, Cech (euclidean), Cubical...\n",
    "- for VR, it is a wrapper on top of custom bindings for ripser.\n",
    "- collapse_edges for reducing the size of the complex,\n",
    "- execution time can be also controlled with the `max_edge_length` parameter\n",
    "- Many parameters: using euclidean distance, but anything from `sklearn.pairwise` is valid; coeff : 2\n",
    ")\n",
    "\n",
    "What is unique about the `giotto-tda` API, is that we calculate the persistence diagrams for lists of point clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagrams = VR.fit_transform(point_clouds)\n",
    "diagrams.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two observations:\n",
    "1. Diagrams are padded with points on the diagonal,\n",
    "2. By default, `reduced_homology==True`, so that the essential point from the diagram in dimension 0 is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(3):\n",
    "    VR.plot(diagrams, sample=n_samples_per_class*ind).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The space of multi-sets in $\\mathbb{R}^2$ lacks the mathematical properties often required for statistical inference. It is customary to derive characteristics from those spaces, like \"persistence entropy\".\n",
    "\n",
    "To be more precise, we view the persistence diagram as a sum of dirac measures supported on its points. The persistence entropy is the entropy of that measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.diagrams import PersistenceEntropy\n",
    "PE = PersistenceEntropy()\n",
    "PE.fit_transform(diagrams[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a RandomForestClassifier on those features. Similarly to `scikit-learn`, we can compose Transformers using pipelines. In `giotto-tda`, there is a dedicated function `make_pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "steps = [VietorisRipsPersistence(homology_dimensions=[0, 1, 2]),\n",
    "         PersistenceEntropy(),\n",
    "         RandomForestClassifier()]\n",
    "\n",
    "pipeline = make_pipeline(*steps)\n",
    "\n",
    "pcs_train, pcs_valid, labels_train, labels_valid = train_test_split(point_clouds, labels)\n",
    "\n",
    "pipeline.fit(pcs_train, labels_train)\n",
    "\n",
    "pipeline.score(pcs_valid, labels_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gravitational waves \n",
    "\n",
    "Gravitational waves (GW) are caused by many cosmic events, such as supernovaes detection, colliding black holes, or neutron stars. While challenging due to their small amplitude, identifying GW is considered as [opening a new window on the universe](https://www.ligo.caltech.edu/page/why-detect-gw). In the literature, formally, the task is often presented as a binary classification problem, which consists of deciding whether a signal contains the signature of a GW or not. Effective approaches with CNNs have been proposed, which, however, require significant data to be trained. \n",
    "\n",
    "In [an article](https://arxiv.org/abs/1910.08245), Chrisopher Bresten and Jae-Hun Jung proposed to include topological signatures in a CNN classifier to improve the models' performance. In this tutorial, we present an example of a classification pipeline relying only on topological features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a setting simpler than in the article. We create a dataset as follows:\n",
    "\n",
    "* Generate gravitational wave signals that correspond to non-spinning binary black hole mergers\n",
    "* Generate a noisy time series and embed a gravitational wave signal with probability 0.5 at a random time.\n",
    "\n",
    "The result is a set of time series of the form\n",
    "\n",
    "$$ s = g + \\epsilon \\frac{1}{R}\\xi $$\n",
    "\n",
    "where $g$ is a gravitational wave signal from the reference set, $\\xi$ is Gaussian noise, $\\epsilon=10^{-19}$ scales the noise amplitude to the signal, and $R$ is a parameter that controls the signal-to-noise-ratio (SNR).\n",
    "\n",
    "Here, we set $R$ to $0.65$, what gives an SNR of 17.89, placing ourselves in a favorable setting, compared to the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.generate_datasets import make_gravitational_waves\n",
    "from pathlib import Path\n",
    "\n",
    "R = 0.65\n",
    "n_signals = 500\n",
    "DATA = Path(\"./data\")\n",
    "\n",
    "noisy_signals, gw_signals, labels = make_gravitational_waves(\n",
    "    path_to_data=DATA, n_signals=n_signals, r_min=R, r_max=R, n_snr_values=5\n",
    ")\n",
    "\n",
    "print(f\"Number of noisy signals: {len(noisy_signals)}\")\n",
    "print(f\"Number of timesteps per series: {len(noisy_signals[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's visualise the two different types of time series that we wish to classify: one that is pure noise vs. one that is composed of noise plus an embedded gravitational wave signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# get the index corresponding to the first pure noise time series\n",
    "background_idx = np.argmin(labels)\n",
    "# get the index corresponding to the first noise + gravitational wave time series\n",
    "signal_idx = np.argmax(labels)\n",
    "\n",
    "ts_noise = noisy_signals[background_idx]\n",
    "ts_background = noisy_signals[signal_idx]\n",
    "ts_signal = gw_signals[signal_idx]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(ts_noise))), y=ts_noise, mode=\"lines\", name=\"noise\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(ts_background))),\n",
    "        y=ts_background,\n",
    "        mode=\"lines\",\n",
    "        name=\"background\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(ts_signal))), y=ts_signal, mode=\"lines\", name=\"signal\"),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make two observations:\n",
    "1. It is hard to distinguish the signal by eye,\n",
    "2. The signal features some regularity or periodicity.\n",
    "\n",
    "Both observations lead us to examining the _**Takens embedding**_ of the signal $s(t)$, in order to pick up the recurrent structure. Indeed, if $f$ is sampled from a dynamical system with a non-trivial recurrent structure, then, for appropriate parameters, the image by the embedding will have non-trivial topology.\n",
    "\n",
    "More formally,, we extract a sequence of vectors in $\\mathbb{R}^{d}$ of the form\n",
    "\n",
    "$$\n",
    "TD_{d,\\tau} s : \\mathbb{R} \\to \\mathbb{R}^{d}\\,, \\qquad t \\to \\begin{bmatrix}\n",
    "           s(t) \\\\\n",
    "           s(t + \\tau) \\\\\n",
    "           s(t + 2\\tau) \\\\\n",
    "           \\vdots \\\\\n",
    "           s(t + (d-1)\\tau)\n",
    "         \\end{bmatrix},\n",
    "$$\n",
    "where $d$ is the embedding dimension and $\\tau$ is the time delay. The quantity $(d-1)\\tau$ is known as the \"window size\" and the difference between $t_{i+1}$ and $t_i$ is called the stride.\n",
    "\n",
    "Let's examine what the time delay embedding of a pure gravitational wave signal looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.time_series import SingleTakensEmbedding\n",
    "embedding_dimension = 30\n",
    "embedding_time_delay = 30\n",
    "stride = 5\n",
    "\n",
    "embedder = SingleTakensEmbedding(\n",
    "    parameters_type=\"search\", n_jobs=6, time_delay=embedding_time_delay, dimension=embedding_dimension, stride=stride\n",
    ")\n",
    "\n",
    "y_gw_embedded = embedder.fit_transform(gw_signals[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use PCA to project our high-dimensional space to 3-dimensions for visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from gtda.plotting import plot_point_cloud\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "y_gw_embedded_pca = pca.fit_transform(y_gw_embedded)\n",
    "\n",
    "plot_point_cloud(y_gw_embedded_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see that the decaying periodic signal generated by a black hole merger emerges as a _spiral_ in the time delay embedding space! For contrast, let's compare this to one of the pure noise time series in our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 30\n",
    "embedding_time_delay = 30\n",
    "stride = 5\n",
    "\n",
    "embedder = SingleTakensEmbedding(\n",
    "    parameters_type=\"search\", n_jobs=6, time_delay=embedding_time_delay, dimension=embedding_dimension, stride=stride\n",
    ")\n",
    "\n",
    "y_noise_embedded = embedder.fit_transform(noisy_signals[background_idx])\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "y_noise_embedded_pca = pca.fit_transform(y_noise_embedded)\n",
    "\n",
    "plot_point_cloud(y_noise_embedded_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, pure noise resembles a high-dimensional ball in the time delay embedding space. Let's see if we can use persistent homology to tease apart which time series contain a gravitational wave signal versus those that don't. To do so we will adapt the strategy from the original article:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing and plotting of the diagrams\n",
    "(for the two classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.diagrams import PersistenceEntropy, Scaler\n",
    "\n",
    "VR = VietorisRipsPersistence(homology_dimensions=[0, 1])\n",
    "dgms = VR.fit_transform([y_gw_embedded_pca, y_noise_embedded_pca])\n",
    "dgms = Scaler().fit_transform(dgms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VR.plot(dgms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VR.plot(dgms, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are slightly more, persistent 1-cycles. Let us see, experimentally, if we can find topological features which distinguish topological signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate 200-dimensional time delay embeddings of each time series\n",
    "2. Use PCA to reduce the time delay embeddings to 3-dimensions\n",
    "3. Use the Vietoris-Rips construction to calculate persistence diagrams of $H_0$ and $H_1$ generators\n",
    "4. Extract feature vectors using persistence entropy\n",
    "5. Train a binary classifier on the topological features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the topological feature generation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do steps 1 and 2 by using the following ``giotto-tda`` tools:\n",
    "\n",
    "- The ``TakensEmbedding`` transformer – instead of ``SingleTakensEmbedding`` – which will transform each time series in ``noisy_signals`` separately and return a collection of point clouds;\n",
    "- ``CollectionTransformer``, which is a convenience \"meta-estimator\" for applying the same PCA to each point cloud resulting from step 1.\n",
    "\n",
    "Using the ``Pipeline`` class from ``giotto-tda``, we can chain all operations up to and including step 4 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.diagrams import PersistenceEntropy, Scaler\n",
    "from gtda.homology import WeakAlphaPersistence\n",
    "from gtda.metaestimators import CollectionTransformer\n",
    "from gtda.pipeline import Pipeline\n",
    "from gtda.time_series import TakensEmbedding\n",
    "\n",
    "embedding_dimension = 200\n",
    "embedding_time_delay = 10\n",
    "stride = 10\n",
    "\n",
    "embedder = TakensEmbedding(time_delay=embedding_time_delay,\n",
    "                           dimension=embedding_dimension,\n",
    "                           stride=stride)\n",
    "\n",
    "batch_pca = CollectionTransformer(PCA(n_components=3), n_jobs=-1)\n",
    "\n",
    "persistence = VietorisRipsPersistence(homology_dimensions=[0, 1], n_jobs=-1)\n",
    "#persistence = WeakAlphaPersistence(homology_dimensions=[0, 1], n_jobs=-1)\n",
    "\n",
    "scaling = Scaler() # PEntropy is invariant to scaling\n",
    "\n",
    "entropy = PersistenceEntropy(normalize=True, nan_fill_value=-10)\n",
    "\n",
    "\n",
    "steps = [(\"embedder\", embedder),\n",
    "         (\"pca\", batch_pca),\n",
    "         (\"persistence\", persistence),\n",
    "         (\"scaling\", scaling),\n",
    "         (\"entropy\", entropy)]\n",
    "topological_transformer = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = topological_transformer.fit_transform(noisy_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final step, let's train a simple classifier on our topological features. As usual we create training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    features, labels, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then fit and evaluate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "def print_scores(fitted_model, X_train, y_train, X_valid, y_valid):\n",
    "    res = {\n",
    "        \"Accuracy on train:\": accuracy_score(fitted_model.predict(X_train), y_train),\n",
    "        \"ROC AUC on train:\": roc_auc_score(\n",
    "            y_train, fitted_model.predict_proba(X_train)[:, 1]\n",
    "        ),\n",
    "        \"Accuracy on valid:\": accuracy_score(fitted_model.predict(X_valid), y_valid),\n",
    "        \"ROC AUC on valid:\": roc_auc_score(\n",
    "            y_valid, fitted_model.predict_proba(X_valid)[:, 1]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    for k, v in res.items():\n",
    "        print(f\"{k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LinearSVM, RandomForest\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print_scores(model, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple baseline, this model is not too bad - it outperforms the deep learning baseline in the article which typically fares little better than random on the raw data. However, the combination of deep learning and persistent homology is where significant performance gains are seen - we leave this as an exercise to the intrepid reader!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topology of gravitational waves\n",
    "\n",
    "It has been shown that topological information contained in the Sliding window embedding of gravitational waves is relevant.\n",
    "\n",
    "We will extend the analysis above by\n",
    "- using a vectorized input, instead of the raw persistence diagram,\n",
    "- interpretting the weights of the classifier, as picking out the topological features of the gravitational waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.diagrams import PersistenceImage, PersistenceLandscape\n",
    "embedder = TakensEmbedding(time_delay=embedding_time_delay,\n",
    "                           dimension=embedding_dimension,\n",
    "                           stride=stride)\n",
    "\n",
    "image = PersistenceImage(sigma=0.1, n_bins=10,)\n",
    "landscape = PersistenceLandscape(n_layers=2, n_bins=10)\n",
    "\n",
    "topological_image_transformer = Pipeline(steps=[*steps[0:-1], (\"img\", image)])\n",
    "topological_landscape_transformer = Pipeline(steps=[*steps[0:-1], (\"landscape\", landscape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#imgs = topological_image_transformer.fit_transform(noisy_signals)\n",
    "lndscps = topological_landscape_transformer.fit_transform(noisy_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lndscps_flat = lndscps.reshape(lndscps.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    lndscps_flat, labels, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LogisticRegression(penalty=\"l1\", max_iter=2000, solver=\"liblinear\", verbose=4)\n",
    "model.fit(X_train, y_train)\n",
    "print_scores(model, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_validate(model, lndscps_flat, labels, cv=KFold(n_splits=10, shuffle=True, random_state=42), return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.concatenate([est.coef_ for est in res['estimator']], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_coefs, std_coefs = np.mean(coefs, axis=0), np.std(coefs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim, n_layers, n_bins = len(landscape.homology_dimensions_), landscape.n_layers, image.n_bins\n",
    "mean_landscape, std_landscape = [c.reshape(1, n_dim*n_layers, n_bins) for c in [mean_coefs, std_coefs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape.plot(mean_lanscape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape.plot(std_landscape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilbert Embedding ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topological_image_transformer = Pipeline(steps=[*steps[0:-1], (\"img\", image)])\n",
    "imgs = topological_image_transformer.fit_transform(noisy_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_flat = imgs.reshape(imgs.shape[0], -1)\n",
    "imgs.shape, imgs_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim, n_bins = len(image.homology_dimensions_), image.n_bins\n",
    "mean_as_img, std_as_img = [c.reshape(1, n_dim, n_bins, n_bins) for c in [mean_coefs, std_coefs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.plot(mean_as_img, homology_dimension_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.plot(std_as_img, homology_dimension_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim, n_bins = len(image.homology_dimensions_), image.n_bins\n",
    "weights_as_img = model.coef_.reshape(1, n_dim, n_bins, n_bins)\n",
    "image.plot(weights_as_img, homology_dimension_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.plot(weights_as_img, homology_dimension_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
